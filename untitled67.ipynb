{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv7wDvoF7Wx7PCQMIj1E3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ctrl408/ViT-implementations/blob/main/untitled67.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSH attention as described in https://openreview.net/pdf?id=rkgNKkHtvB\n",
        "# adapted from trax, stripped to what paper said needed to work\n",
        "# namely that buckets need to be at least 64 with 8 rounds of hashing\n",
        "# https://github.com/google/trax/blob/master/trax/layers/research/efficient_attention.py#L442\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "def make_unit_length(x, epsilon=1e-6):\n",
        "    norm = x.norm(p=2, dim=-1, keepdim=True)\n",
        "    return x.div(norm + epsilon)\n",
        "\n",
        "def sort_key_val(t1, t2, dim=-1):\n",
        "    values, indices = t1.sort(dim=dim)\n",
        "    t2 = t2.expand_as(t1)\n",
        "    return values, t2.gather(dim, indices)\n",
        "\n",
        "def batched_index_select(values, indices):\n",
        "    b = values.shape[0]\n",
        "    return values[torch.arange(0, b), indices.transpose(0, 1)].transpose(0, 1)\n",
        "\n",
        "class LSHAttention(nn.Module):\n",
        "    def __init__( self,\n",
        "                  dropout = 0.,\n",
        "                  bucket_size = 64,\n",
        "                  n_hashes = 8,\n",
        "                  allow_duplicate_attention = False,\n",
        "                  attend_across_buckets = False,\n",
        "                  rehash_each_round = True,\n",
        "                  drop_for_hash_rate = 0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        if dropout >= 1.0:\n",
        "            raise ValueError('Dropout rates must be lower than 1.')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n",
        "\n",
        "        assert rehash_each_round or allow_duplicate_attention, (\n",
        "            'The setting {allow_duplicate_attention=False, rehash_each_round=False}'\n",
        "            ' is not implemented.')\n",
        "\n",
        "        self.n_hashes = n_hashes\n",
        "        self.bucket_size = bucket_size\n",
        "\n",
        "        self._allow_duplicate_attention = allow_duplicate_attention\n",
        "        self._attend_across_buckets = attend_across_buckets\n",
        "        self._rehash_each_round = rehash_each_round\n",
        "\n",
        "    def _sample_rotation(self, shape, vecs):\n",
        "        device = vecs.device\n",
        "        return torch.randn(shape, device=device)\n",
        "\n",
        "    def hash_vectors(self, n_buckets, vecs):\n",
        "        batch_size = vecs.shape[0]\n",
        "        device = vecs.device\n",
        "\n",
        "        # See https://arxiv.org/pdf/1509.02897.pdf\n",
        "        # We sample a different random rotation for each round of hashing to\n",
        "        # decrease the probability of hash misses.\n",
        "        assert n_buckets % 2 == 0\n",
        "\n",
        "        rot_size = n_buckets\n",
        "\n",
        "        rotations_shape = (\n",
        "            vecs.shape[-1],\n",
        "            self.n_hashes if self._rehash_each_round else 1,\n",
        "            rot_size // 2)\n",
        "\n",
        "        random_rotations = self._sample_rotation(rotations_shape, vecs)\n",
        "\n",
        "        dropped_vecs = self.dropout_for_hash(vecs)\n",
        "        rotated_vecs = torch.einsum('btf,fhi->bhti', dropped_vecs, random_rotations)\n",
        "\n",
        "        if self._rehash_each_round:\n",
        "            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n",
        "            buckets = torch.argmax(rotated_vecs, axis=-1)\n",
        "            # buckets is now (self.n_hashes, seqlen). Next we add offsets so that\n",
        "            # bucket numbers from different hashing rounds don't overlap.\n",
        "            offsets = torch.arange(self.n_hashes, device=device)\n",
        "            offsets = torch.reshape(offsets * n_buckets, (1, -1, 1))\n",
        "            buckets = torch.reshape(buckets + offsets, (batch_size, -1,))\n",
        "        else:\n",
        "            assert not self._factorize_hash\n",
        "            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n",
        "            # In this configuration, we map each item to the top self.n_hashes buckets\n",
        "            rotated_vecs = torch.squeeze(rotated_vecs, 0)\n",
        "            bucket_range = torch.arange(0, rotated_vecs.shape[-1], device=device)\n",
        "            bucket_range = torch.reshape(bucket_range, (1, -1))\n",
        "            bucket_range = bucket_range.expand_as(rotated_vecs.shape)\n",
        "\n",
        "            _, buckets = sort_key_val(rotated_vecs, bucket_range, dim=-1)\n",
        "            buckets = buckets[:, -self.n_hashes:]\n",
        "\n",
        "            h, *_ = buckets.shape\n",
        "            buckets = torch.reshape(buckets.permute((*_, h)), (-1,))\n",
        "\n",
        "        return buckets\n",
        "\n",
        "    def forward(self, qk, v):\n",
        "        batch_size, seqlen, _ = qk.shape\n",
        "        device = qk.device\n",
        "\n",
        "        n_buckets = seqlen // self.bucket_size\n",
        "        n_bins = n_buckets\n",
        "\n",
        "        buckets = self.hash_vectors(n_buckets, qk)\n",
        "        # We use the same vector as both a query and a key.\n",
        "        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n",
        "\n",
        "        ticker = torch.arange(0, self.n_hashes * seqlen, device=device).unsqueeze(0)\n",
        "        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n",
        "        buckets_and_t = buckets_and_t.detach()\n",
        "\n",
        "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
        "        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n",
        "        _, undo_sort = sort_key_val(sticker, ticker, dim=-1)\n",
        "\n",
        "        sbuckets_and_t = sbuckets_and_t.detach()\n",
        "        sticker = sticker.detach()\n",
        "        undo_sort = undo_sort.detach()\n",
        "\n",
        "        st = (sticker % seqlen)\n",
        "        sqk = batched_index_select(qk, st)\n",
        "        sv = batched_index_select(v, st)\n",
        "\n",
        "        # Split off a \"bin\" axis so that attention only occurs within chunks.\n",
        "        bq_t = bkv_t = torch.reshape(st, (batch_size, self.n_hashes * n_bins, -1))\n",
        "        bqk = torch.reshape(sqk, (batch_size, self.n_hashes * n_bins, -1, sqk.shape[-1]))\n",
        "        bv = torch.reshape(sv, (batch_size, self.n_hashes * n_bins, -1, sv.shape[-1]))\n",
        "        bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, self.n_hashes * n_bins, -1))\n",
        "\n",
        "        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n",
        "        # fine because they effectively provide a learnable temperature for the\n",
        "        # attention softmax, but normalizing keys is needed so that similarity for\n",
        "        # the purposes of attention correctly corresponds to hash locality.\n",
        "        bq = bqk\n",
        "        bk = make_unit_length(bqk)\n",
        "\n",
        "        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
        "        # boundaries might occur in the middle of a sequence of items from the\n",
        "        # same bucket, so this increases the chances of attending to relevant items.\n",
        "        def look_one_back(x):\n",
        "            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n",
        "            return torch.cat([x, x_extra], dim=2)\n",
        "\n",
        "        bk = look_one_back(bk)\n",
        "        bv = look_one_back(bv)\n",
        "        bkv_t = look_one_back(bkv_t)\n",
        "        bkv_buckets = look_one_back(bkv_buckets)\n",
        "\n",
        "        # Dot-product attention.\n",
        "        dots = torch.einsum('bhie,bhje->bhij', bq, bk) / (bq.shape[-1] ** -0.5)\n",
        "\n",
        "        # Causal masking\n",
        "        mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n",
        "        dots = dots - 1e9 * mask\n",
        "\n",
        "        # Mask out attention to self except when no other targets are available.\n",
        "        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n",
        "        dots = dots - 1e5 * self_mask\n",
        "\n",
        "        # Mask out attention to other hash buckets.\n",
        "        if not self._attend_across_buckets:\n",
        "            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n",
        "            dots = dots - 1e7 * bucket_mask\n",
        "\n",
        "        # Don't double-count query-key pairs across multiple rounds of hashing.\n",
        "        # There are two possible strategies here. (1) The default is to count how\n",
        "        # many times a query-key pair is repeated, and to lower its log-prob\n",
        "        # correspondingly at each repetition. (2) When hard_k is set, the code\n",
        "        # instead masks all but the first occurence of each query-key pair.\n",
        "        if not self._allow_duplicate_attention:\n",
        "            locs1 = undo_sort // bq_t.shape[-1]\n",
        "            locs2 = (locs1 + 1) % (self.n_hashes * n_bins)\n",
        "            if not self._attend_across_buckets:\n",
        "                locs1 = buckets * (self.n_hashes * n_bins) + locs1\n",
        "                locs2 = buckets * (self.n_hashes * n_bins) + locs2\n",
        "            locs = torch.cat([\n",
        "                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n",
        "                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n",
        "            ], 1).permute((0, 2, 1))\n",
        "\n",
        "            slocs = batched_index_select(locs, st)\n",
        "            b_locs = torch.reshape(slocs, (batch_size, self.n_hashes * n_bins, -1, 2 * self.n_hashes))\n",
        "\n",
        "            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n",
        "\n",
        "            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n",
        "            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n",
        "            bkv_locs = look_one_back(b_locs)\n",
        "\n",
        "            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :]).float().sum(dim=-1)\n",
        "            dup_counts = dup_counts.detach()\n",
        "            assert dup_counts.shape == dots.shape\n",
        "            dots = dots - torch.log(dup_counts + 1e-9)\n",
        "\n",
        "        # Softmax.\n",
        "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
        "        dots = torch.exp(dots - dots_logsumexp)\n",
        "        dots = self.dropout(dots)\n",
        "\n",
        "        bo = torch.einsum('buij,buje->buie', dots, bv)\n",
        "        so = torch.reshape(bo, (batch_size, -1, bo.shape[-1]))\n",
        "        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n",
        "\n",
        "        o = batched_index_select(so, undo_sort)\n",
        "        _, logits = sort_key_val(sticker, slogits, dim=-1)\n",
        "\n",
        "        if self.n_hashes == 1:\n",
        "            out = o\n",
        "        else:\n",
        "            o = torch.reshape(o, (batch_size, self.n_hashes, seqlen, o.shape[-1]))\n",
        "            logits = torch.reshape(logits, (batch_size, self.n_hashes, seqlen, 1))\n",
        "            probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdims=True))\n",
        "            out = torch.sum(o * probs, dim=1)\n",
        "\n",
        "        assert out.shape == v.shape\n",
        "        return out\n",
        "\n",
        "class LSHSelfAttention(nn.Module):\n",
        "    def __init__(self, emb, heads = 8, bucket_size = 64, n_hashes = 8, **kwargs):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "\n",
        "        self.toqk = nn.Linear(emb, emb * heads)\n",
        "        self.tov = nn.Linear(emb, emb * heads)\n",
        "        self.unify_heads = nn.Linear(emb * heads, emb)\n",
        "\n",
        "        self.bucket_size = bucket_size\n",
        "        self.lsh_attn = LSHAttention(bucket_size=bucket_size, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, e, h = *x.shape, self.heads\n",
        "        assert t % self.bucket_size == 0, f'Sequence length needs to be divisible by target bucket size - {self.bucket_size}'\n",
        "\n",
        "        qk = self.toqk(x)\n",
        "        v = self.tov(x)\n",
        "\n",
        "        def merge_heads(v):\n",
        "            return v.view(b, t, h, e).transpose(1, 2).reshape(b * h, t, e)\n",
        "\n",
        "        def split_heads(v):\n",
        "            return v.view(b, h, t, e).transpose(1, 2).contiguous()\n",
        "\n",
        "        qk = merge_heads(qk)\n",
        "        v = merge_heads(v)\n",
        "        attn_out = self.lsh_attn(qk, v)\n",
        "        out = split_heads(attn_out).view(b, t, h * e)\n",
        "\n",
        "        return self.unify_heads(out)"
      ],
      "metadata": {
        "id": "xCa_mqdPfo_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "E0UXBOHXe1NL",
        "outputId": "04688b53-a524-45a5-e0c1-6ce9562c5147"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LSHSelfAttention' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe5bba8f1c3c>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Create LSH and Standard Attention Layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mlsh_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSHSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hashes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mstandard_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LSHSelfAttention' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "\n",
        "# LSH Attention and LSHSelfAttention classes are assumed to be defined as provided\n",
        "\n",
        "# Define standard multi-head attention for comparison\n",
        "class StandardAttention(nn.Module):\n",
        "    def __init__(self, emb, heads=8):\n",
        "        super().__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=emb, num_heads=heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length, emb_dim)\n",
        "        # We need to permute x to shape (seq_length, batch_size, emb_dim) as required by nn.MultiheadAttention\n",
        "        x = x.permute(1, 0, 2)\n",
        "        attn_output, _ = self.multihead_attn(x, x, x)\n",
        "        # Permute back to original shape\n",
        "        return attn_output.permute(1, 0, 2)\n",
        "\n",
        "def benchmark_attention(attn_layer, tokens, num_runs=10):\n",
        "    # Warm-up run\n",
        "    _ = attn_layer(tokens)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        _ = attn_layer(tokens)\n",
        "    end_time = time.time()\n",
        "\n",
        "    avg_time = (end_time - start_time) / num_runs\n",
        "    return avg_time\n",
        "\n",
        "# Parameters\n",
        "depth = 6\n",
        "seqlen = 40960\n",
        "emb = 64\n",
        "batch_size = 12\n",
        "\n",
        "# Create LSH and Standard Attention Layers\n",
        "lsh_attention = LSHSelfAttention(emb, heads=8, bucket_size=64, n_hashes=8)\n",
        "standard_attention = StandardAttention(emb, heads=8)\n",
        "\n",
        "# Generate input tokens\n",
        "tokens = torch.randn((batch_size, seqlen, emb))\n",
        "# Benchmark LSH Attention\n",
        "lsh_time = benchmark_attention(lsh_attention, tokens)\n",
        "print(f\"Average time for LSH Attention: {lsh_time:.6f} seconds\")\n",
        "\n",
        "# Benchmark Standard Attention\n",
        "standard_time = benchmark_attention(standard_attention, tokens)\n",
        "print(f\"Average time for Standard Attention: {standard_time:.6f} seconds\")\n"
      ]
    }
  ]
}